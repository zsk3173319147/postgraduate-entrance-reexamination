### 基于GPT2模型的中文小说续写



#### GPT的全称为Generative Pre-trained Transformer

#### 项目名称：基于GPT-2的中文小说续写项目

**项目描述**：本项目是一个基于GPT-2的中文小说续写项目。项目目标是利用预训练的GPT-2模型来生成连续的中文小说文本，使其看起来像是由一个人写成的一样。通过此项目，我学习了GPT-2模型的原理、数据预处理、模型训练和生成技术。

**项目职责**：

- 下载和整理了大量中文小说数据使用Python编写了数据预处理代码，包括分词、文本清洗、转换成GPT-2模型所需的格式等
- 根据GPT-2模型的原理，调整了模型的一些参数，如学习率、批次大小、模型层数等
- 在GPU服务器上训练了GPT-2模型，并使用训练好的模型来生成中文小说续写文本

**项目成果**：

- 成功生成了一些看起来像是由一个人写成的中文小说续写文本，并且有一定的可读性

- 实现了数据预处理、模型训练和生成的流程，并成功生成了中文文章。

- 熟悉了GPT-2模型和transformer架构，并对模型的关键参数进行了调参。

  



1. #### 该项目的背景和目的是什么？

   > 这个项目基于GPT-2模型，旨在实现自动续写中文小说的功能。中文小说是中国传统文学的重要组成部分，受到了广大读者的热爱。但是，中文小说的创作需要耗费大量的人力和时间，因此自动续写中文小说的技术应运而生，能够极大地提高中文小说创作的效率。此项目的目的是探究基于GPT-2的中文小说续写技术，提高中文小说的创作效率和质量。

2. #### 您是如何构建和训练基于GPT2的语言模型的？使用了哪些技术？

   > 我使用了Python编程语言和PyTorch深度学习框架，构建了基于GPT-2的中文小说续写模型。在模型训练过程中，我从互联网上采集了一些经典中文小说的文本数据，并进行了数据清洗和预处理。然后，我使用了GPT-2模型对这些数据进行了训练，并对模型进行了优化和调参，以获得更好的续写效果。

3. #### 在该项目中，您主要负责了哪些工作？其中遇到了哪些困难和挑战？

   > 这个项目是我学习别人在github开源项目做的课程设计，我学习了该开源项目的代码实现、数据处理、模型训练等方面，并根据自己的理解和需要做了适当的修改和调整。遇到的困难有文件下载后转换的编码问题、怎么会转换成可以使用的json格式的文件，同时在训练的时候由于本地机的训练资源不足，转而使用colab的云服务资源，最大的问题是在训练自己的模型时，调参出现的问题。比如开始我的loss下降很慢，之后我发现每次训练都是从头开始，我觉得这样很浪费时间，就训练模型改成了上次最后训练完成的，后来我的loss不断在3和5之间震荡，怎么也下不去，想了一会，查了相关资料，可能有
   >
   > 学习率过大：如果学习率设置过大，模型参数更新时可能会“跳过”最优解，导致损失函数震荡。你可以尝试降低学习率。
   >
   > 数据集问题：如果数据集中存在错误标注或噪声数据，也可能导致损失函数震荡。你可以检查数据集并进行清洗。
   >
   > 模型问题：如果模型结构不合理或存在 bug，也可能导致损失函数震荡。你可以检查模型结构并修复 bug。
   >
   > 优化器问题：如果使用的优化器不合适，也可能导致损失函数震荡。你可以尝试更换优化器。
   >
   > 这四种可能。

   #### 在数据预处理过程中，你是如何进行文本清洗和去重的？

   > 首先把爬虫得到的txt文件，按照章节转换成字符串列表形式，之后转换成json文件 命名为train.json
   >
   > 在把json文件里的列表元素取出来，它是一个元素就是一个章节，之后把换行符\n换成SEP，每一段开头添加MASK表示开始，在结尾添加CLS表示结束
   >
   > 之后使用thulac的工具包进行分词，创建词汇表，默认大小为5万，对分词后的文本进行拟合，并获取其索引到单词的映射表。然后，它创建一个列表，包含预定义的5个特殊标记和映射表中的所有单词。分别是这五个特殊标记是`[SEP]`、`[CLS]`、`[MASK]`、`[PAD]`和`[UNK]`。
   >
   > - `[SEP]`：分隔符，用于分隔两个句子或段落。
   > - `[CLS]`：分类符号，通常用于分类任务中，表示句子或段落的开头。
   > - `[MASK]`：掩码符号，用于遮盖某些单词，以便模型可以预测被遮盖的单词。
   > - `[PAD]`：填充符号，用于将不同长度的句子填充到相同长度。
   > - `[UNK]`：未知符号，用于表示未出现在词汇表中的单词。
   >
   > 转换为词汇表vocab后就可以使用BERT的Tokenizer进行词向量化，保存到txt文件中
   >
   > 之后将词向量输入GPT-2 进行训练 并设置相应参数，等待训练结果完成 直接看generate生成

   #### GPT-2模型有很多参数需要调整，你是如何选择最优的参数配置的？

   >这个需要大量时间试错、查资料，之后我也向老师问过一些参数的实际调整效果
   >
   >比如：。。。。。

   #### 在生成中文小说续写文本时，你是如何保证生成文本的连贯性和逻辑性的？

   > 最基本的你的数据集要规整，分词后的结果要精确，之后训练的时候要调必要参数参数 学习率 层数 词嵌入的维度 注意力头的个数 输入序列的长度 
   >
   > 之后生成的时候也有参数需要调 比如topk temporture dropout

   #### 你对于生成的文本质量进行了评估，具体采用了哪些评估指标？是否有其他评估指标可以使用？

   > 在训练的时候首先就会看loss值，之后观察loss值变化 调整参数 或者优化数据集 之后使用BLEU这个评价指标来对生成文本进行评价  它是一种度量生成式文本和参考文本的相似度的指标 虽然pytorch并没有内置其实现 但是可以引入NLTK库进行计算 这个值其实参考意义不是决定性的 只能反应表面相似度 至于逻辑和多样性没有体现
   >
   > 此外我编写了一个generate函数，接收一些参数 之后生成样本 我会人工进行评估

   #### 你在项目中遇到了哪些挑战和困难？你是如何解决这些问题的？

   > 很多困难 环境配置 文本处理 模型调参 

   #### 你对于自动生成文本任务的未来发展有何看法？你认为这个领域还存在哪些值得研究的问题？

   > 



#### GPT-2 模型由多层单向 Transformer 的解码器部分构成，本质上是自回归模型，自回归的意思是指，每次产生新单词后，将新单词加到原输入句后面，作为新的输入句。 GPT-2 只会考虑在待预测词位置左侧的词对待预测词的影响。



1. **语料库是一行一段话，从train.py中的build_files函数可以看到‘\n’被替换成[SEP]，[SEP],[MASK]是MML模型使用的token，weibo.txt就是这样子的结构；**
2. **首先生成词汇表，使用cache/make_vocab.py，作者使用thulac进行分词，其实也可以用jieba(lines[i]=' '.join(list(jieba.cut(line)))，修改第22行：`lines=f.readlines()`;注意`--vocab_size`参数，这个参数会在后面用到；**

#### 需要考虑的超参数	

1. `n_layer`：模型的层数，决定了模型的复杂度，通常为12、24或36层。
2. `n_head`：模型中多头注意力的头数，决定了模型可以同时关注多少个位置，通常为8或16个头。
3. `d_model`：模型中的嵌入向量的维度，决定了模型的表达能力，通常为768或1024。
4. `d_ff`：模型中全连接层的维度，决定了模型中间层的大小，通常为4*d_model。
5. `n_ctx`：模型输入序列的最大长度，通常为1024或2048。
6. `batch_size`：每次训练时使用的样本数，通常为16、32、64等。
7. `learning_rate`：学习率，控制模型参数更新的速度。
8. `num_train_steps`：训练步数，通常为几十万到数百万步不等。



#### 1. 模型的收敛取决于词嵌入的维度，维度越大收敛越快越好。

#### 2.head与隐藏层数可以适当裁剪，隐藏层可以设置高一些，multi-head感觉超过5层之后似乎对于生成的结果影响并不大。

#### 3. 模型长度不影响训练，但与学习效果有很大关联，能大些就大些。

#### 4.训练效率问题，模型参数与训练效率息息相关，合理的batch数量、GPU显存、适当的模型参数（multi-head拉低，layer尽量比head大），100%使用率还是能达到的，当然了，小数据集随便跑吧，不用纠结，反正不费多少时间

#### 什么是语言模型？它的作用是什么？

> 语言模型（Language Model）是一种用于自然语言处理（NLP）的基本技术，它的主要作用是预测给定的一段文本中下一个可能出现的单词或字符，以此来衡量这段文本的语言质量。它可以被用来评估文本的连贯性、准确性、完整性等，同时也可以用于文本生成、自动翻译、语音识别、机器阅读理解等领域。语言模型是许多NLP任务的基础和核心，能够有效地提高自然语言处理的精度和效率。

#### 训练过程中的一些问题

1. ##### 我使用很小的数据集，之后由于没有cuda，使用cpu训练，训练200轮，loss也降到了0.03左右但是测试生成文章，全是乱码，这是为什么？

   > 如果你在训练过程中发现 loss 降到了一个较低的水平，但是在测试生成文章时却出现了乱码，可能有以下几个原因：
   >
   > 1. 过拟合：由于你使用的是一个很小的数据集，模型可能过度拟合训练数据，导致在测试数据上表现不佳。
   >
   > 2. 数据预处理问题：如果你在训练和测试时使用了不同的数据预处理方法，可能会导致模型无法正确解码生成的文本。
   >
   > 3. 模型结构或参数问题：如果模型结构或参数设置不当，可能会导致模型无法正确学习到文本生成的规律
   >
   >    我觉得1、和3有可能，而是不可能的，之后我得使用cuda训练大一点的数据集

2. 减少模型的层数（n_layer）可以减少模型的参数量。同时，减少 batch_size 可能需要增加 epoch 数量才能达到相同的训练效果。这两种方法都有可能减少过拟合（overfitting）的可能性。但是，具体的学习效果需要通过多次执行训练来观察，研究成效难以立竿见影

3. ##### 我的loss一直保持在3下不去？

   > 1. 数据集问题：可能数据集不够大或不够丰富，可以考虑增加数据量或使用更好的数据集；
   > 2. 模型结构问题：模型可能太简单，可以考虑增加模型的层数、宽度、改变激活函数等来提高模型复杂度；
   > 3. 参数调整问题：可以逐步减小学习率来帮助模型跳出局部最优解，或者调整其他超参数如batch_size、dropout等来提高模型的泛化能力；
   > 4. 训练策略问题：可以考虑使用更先进的优化器如AdamW、LAMB等，或者使用一些正则化方法如weight decay、early stopping等来防止过拟合。



#### 你的模型是如何生成中文小说的？

> 我的模型是基于GPT-2的中文小说生成模型，它的工作原理是利用前面的文本来预测下一个单词或字符，然后不断迭代生成下一个单词或字符，直到达到预设的生成长度或者生成出特定的结束标识符。
>
> 具体地说，当输入一段文本作为前缀时，模型会通过计算前缀与各个单词或字符之间的概率来预测下一个单词或字符，生成一个概率分布。接着，模型会根据这个概率分布来进行采样，从而得到下一个单词或字符，并将其作为下一步的输入，不断迭代生成直到达到预设的生成长度或结束标识符。
>
> 在生成过程中，我还可以控制模型的生成温度、top k采样等超参数，以控制模型生成文本的多样性和准确性。
>
> **生成温度是什么意思**？
>
> > 在基于语言模型的生成任务中，生成温度（temperature）是一个用来控制生成文本多样性的超参数。它决定了在每个时间步上，模型输出概率分布中，被选择的下一个字符的随机程度。当温度值较低时，模型更倾向于选择概率较大的字符，生成的文本相对保守和重复性较高；而当温度值较高时，模型更容易选择概率较小的字符，生成的文本更加多样化和创新性，但也可能更加不可预测。因此，在实际应用中，需要根据具体的生成任务和数据集特点，选择适当的温度值来平衡多样性和可控性。
>
> **那topk是什么？**
>
> Top-k是指在生成文本时，只考虑概率最高的前k个词。





#### 在 PyTorch 中，模型通常由两部分组成：模型架构和模型参数。

- 模型架构定义了模型的计算流程，即如何根据输入数据计算输出结果。它由多个层和操作组成，每个层都有自己的计算方法。
- 模型参数是模型中可学习的权重和偏置。它们在训练过程中不断更新，以最小化损失函数。

#### 简单介绍一下transformer

> Transformer是一种用于序列到序列学习的深度学习模型，主要用于自然语言处理（NLP）任务，例如机器翻译、文本摘要、对话生成等。它是由谷歌在2017年提出的，并在其机器翻译系统中取得了显著的成果。
>
> Transformer 的核心思想是自注意力机制（self-attention），它能够在输入序列的所有位置上同时计算每个位置的表示。这种机制可以将每个位置的表示与其他位置的表示相关联，以便在编码和解码阶段捕获更丰富的语义信息。
>
> 在 Transformer 中，有两个主要的模块：编码器和解码器。编码器将输入序列转换为上下文相关的表示，解码器根据编码器的输出和当前的状态生成输出序列。
>
> 每个编码器和解码器都由多个相同的层组成，每个层都包括两个子层：一个是多头自注意力子层，另一个是全连接前馈子层。
>
> 在多头自注意力子层中，输入序列中的每个位置都会与所有其他位置进行关联。它使用多个注意力头来计算注意力矩阵，以便在不同的表示空间中捕获不同的语义信息。
>
> 在全连接前馈子层中，每个位置的表示都通过两个线性变换和激活函数进行转换。这个子层是为了在每个位置上学习一个非线性函数，以便更好地捕获语义信息。
>
> 在解码器中，还有一个额外的注意力子层，称为编码器-解码器注意力子层，用于在生成每个输出时对输入序列进行关注



#### 什么是自注意力机制

> 自注意力机制是Transformer模型的一个关键组成部分，也是它的创新之处之一。简单来说，自注意力机制是一种基于输入序列自身的注意力机制。
>
> 在自注意力机制中，每个输入序列的元素（比如句子中的每个单词）都会被表示为一个向量，该向量将被用于计算与其他向量之间的相似度。这个相似度可以看作是每个元素与整个序列的关联程度。使用这个关联程度，我们可以根据输入序列中的其他元素来加权计算每个元素的表示，从而捕获全局信息。
>
> 在Transformer中，自注意力机制被用于同时计算输入序列的编码和解码。通过多层自注意力机制和前馈神经网络，Transformer模型能够在不引入循环或卷积操作的情况下，对序列数据进行建模和处理。这使得Transformer模型能够在并行计算中更有效地处理长序列数据，



#### GPT-2模型是什么？它有哪些特点和优势？

> GPT-2是一种基于深度学习的语言模型，全称为Generative Pre-trained Transformer 2。它是OpenAI公司在2019年发布的一个预训练模型，采用了Transformer网络结构。GPT-2模型可以对一段文本进行预测，给出下一个可能的单词或词组，从而实现对文本的自动生成。
>
> GPT-2模型的主要特点和优势包括：
>
> 1. 预训练模型：GPT-2模型采用预训练的方式，先在大规模语料库上进行训练，然后通过微调等方式应用到具体的任务中。
> 2. Transformer网络结构：GPT-2模型采用了Transformer网络结构，该结构能够自适应地对输入文本进行编码，同时利用多头注意力机制来捕捉文本中的长程依赖关系，从而提高了模型的性能。
> 3. 多样性生成：GPT-2模型能够在一定程度上实现文本的多样性生成，即在给定的前缀下，生成多个不同的、但语法和语义都合理的后续文本。
> 4. 零样本学习：GPT-2模型可以实现零样本学习，即在没有任何额外训练数据的情况下，生成与训练数据相似的文本。





#### 怎么评价你的模型的好坏？

- Perplexity：可以通过对测试数据集运行模型并计算每个句子的交叉熵得到。最终的perplexity是交叉熵的指数，用来表示模型对于新数据的预测能力。具体计算方法可以参考：https://en.wikipedia.org/wiki/Perplexity
- BLEU：是机器翻译任务中广泛使用的指标，可以用来评估生成文本的质量。它通过比较生成文本和参考文本之间的n-gram重叠程度来计算。具体计算方法可以参考：https://en.wikipedia.org/wiki/BLEU
- ROUGE：也是机器翻译任务中常用的指标之一，可以用来评估生成文本和参考文本之间的相似度。具体计算方法可以参考：https://en.wikipedia.org/wiki/ROUGE_(metric)
- F1 Score：在一些分类任务中常用的指标，可以用来评估模型在预测正例和负例时的准确率和召回率的平衡情况。具体计算方法可以参考：https://en.wikipedia.org/wiki/F1_score

#### 在基于GPT-2的中文小说续写项目中，模型评估和优化技术是非常重要的一环。下面介绍一些常用的技术：

1. 损失函数：损失函数是用来衡量模型预测结果与实际结果之间的差距，常见的损失函数有交叉熵、均方误差、对数损失等。选择合适的损失函数可以提高模型的预测精度。
2. 评价指标：评价指标用来衡量模型预测结果的好坏，如准确率、召回率、F1值等。选择合适的评价指标可以更直观地反映模型的效果。
3. 交叉验证：交叉验证是一种通过对数据集进行多次划分训练和测试来评估模型性能的方法。交叉验证可以充分利用数据集，避免过拟合和欠拟合等问题。
4. 正则化：正则化是一种通过加入正则项来限制模型复杂度的方法，如L1正则化、L2正则化等。正则化可以避免模型过拟合和提高模型泛化能力。
5. 超参数优化：超参数是指模型中需要手动设置的参数，如学习率、批大小、隐藏层数等。通过对超参数进行优化，可以提高模型的性能。
6. 梯度下降算法：梯度下降是一种优化算法，通过反向传播计算梯度来更新模型参数，以最小化损失函数。不同的梯度下降算法有不同的优缺点，需要根据具体情况选择合适的算法。



#### BERT和GPT区别

- GPT 选择 Transformer 里的 **Decoder**，训练目标为一般的语言模型，预测下个字
- BERT 选择 Transformer 里的 **Encoder**，训练目标则为克漏字填空以及下句预测





#### 神经网络中Batch Size的理解

> 直观的理解：
> Batch Size定义：一次训练所选取的样本数。
> Batch Size的大小影响模型的优化程度和速度。同时其直接影响到GPU内存的使用情况，假如你GPU内存不大，该数值最好设置小一点。
>
> 为什么要提出Batch Size？
> 在没有使用Batch Size之前，这意味着网络在训练时，是一次把所有的数据（整个数据库）输入网络中，然后计算它们的梯度进行反向传播，由于在计算梯度时使用了整个数据库，所以计算得到的梯度方向更为准确。但在这情况下，计算得到不同梯度值差别巨大，难以使用一个全局的学习率，所以这时一般使用Rprop这种基于梯度符号的训练算法，单独进行梯度更新。
> 在小样本数的数据库中，不使用Batch Size是可行的，而且效果也很好。但是一旦是大型的数据库，一次性把所有数据输进网络，肯定会引起内存的爆炸。所以就提出Batch Size的概念。
>
> Batch Size设置合适时的优点：
> 1、通过并行化提高内存的利用率。就是尽量让你的GPU满载运行，提高训练速度。
> 2、单个epoch的迭代次数减少了，参数的调整也慢了，假如要达到相同的识别精度，需要更多的epoch。
> 3、适当Batch Size使得梯度下降方向更加准确。
>
> Batch Size从小到大的变化对网络影响
> 1、没有Batch Size，梯度准确，只适用于小样本数据库
> 2、Batch Size=1，梯度变来变去，非常不准确，网络很难收敛。
> 3、Batch Size增大，梯度变准确，
> 4、Batch Size增大，梯度已经非常准确，再增加Batch Size也没有用
>
> 注意：Batch Size增大了，要到达相同的准确度，必须要增大epoch。



#### SGD (Stochastic Gradient Descent) 和 Adam (Adaptive Moment Estimation) 是两种常用的梯度下降算法。

SGD 是一种基本的梯度下降算法，它使用每个样本的梯度来更新模型参数，每次只更新一个样本。这种方法需要大量的迭代才能获得最佳结果，而且可能会陷入局部最小值，收敛速度较慢。但是，SGD 计算速度快，内存占用少，在大规模数据集上表现较好。

Adam 是一种适应性优化算法，它综合了梯度下降和动量优化的优点，它会计算不同维度的自适应学习率来更新模型参数。它能够应对不同的梯度不一样的情况，同时也可以跳出局部极小值。Adam 算法的收敛速度比较快，对于大型数据集的优化效果较好。

总的来说，SGD 适用于小数据集，Adam 适用于大规模数据集。在训练深度神经网络时，通常使用 Adam 算法，因为它能够更快地达到最佳结果。但是，我们在实际应用中要根据具体情况选择合适的算法。



#### Adam（Adaptive Moment Estimation）是一种自适应优化算法，它结合了动量法和自适应梯度的优点。其基本原理如下：

\1. 计算梯度：计算损失函数对于每个参数的梯度。

\2. 计算动量：使用动量（momentum）来计算误差函数的加权平均梯度，减少梯度下降的震荡。

\3. 计算梯度更新：Adam 算法使用一阶矩估计（mean）和二阶矩估计（variance）来计算每个参数的自适应学习率。这两个估计量构成了梯度的变化率，混合了一阶和二阶的信息。

\4. 更新梯度：使用计算得到的自适应学习率和动量来更新每个参数的值。

Adam 算法采用指数加权移动平均方法来计算梯度的一阶和二阶矩估计，使其具有自适应调节学习率的能力。在每次迭代时，Adam 根据计算出来的均值和方差自适应地调整每个参数的学习率。并且，因为参数的均值和方差是通过指数加权平均的方式进行更新，所以使得该算法具有较好的鲁棒性，能够处理一些噪声和不可预知的变化。



#### 在机器学习中，动量（momentum）是指梯度下降优化算法中的一种方法，用于控制梯度下降的速度和方向。

动量的基本思想是，在梯度下降中引入历史梯度的加权平均，可以减少梯度下降的震荡和收敛时间，从而更快地找到最优解。

具体来说，动量方法计算出一个动量向量 m，其中 m 是历史梯度向量的加权平均值。动量向量有助于加快梯度下降的速度，并使其朝着收敛的方向移动。更具体地说，我们可以使用下面的式子来计算动量向量：

m = β * m + (1-β) * g

其中 g 是当前的梯度向量，β 是动量的超参数，可以控制历史梯度与当前梯度的比率，一般取值在 0.8-0.99 之间。在迭代过程中，动量向量 m 会不断地更新，然后根据动量向量和学习率来调整权重值，从而更新模型。

总的来说，动量方法可以使梯度下降更加平稳和快速，特别是在复杂的高维空间中。同时，动量还可以帮助模型跳出小的局部最小值，从而更有可能找到全局最小值。因此，它被广泛应用于训练神经网络和其他机器学习模型。



### pytorch



#### 什么是PyTorch？

PyTorch是一个基于Python的科学计算包，它主要有两个用途：

- 类似Numpy但是能利用GPU加速
- 一个非常灵活和快速的用于深度学习的研究平台



卷积适用于 一个系统 输入不稳定 输出稳定 利用卷积求系统的存量

周围像素点对中间的影响

过滤器的卷积核 



``` python
 import torch
     \# 从列表转换为 tensor
a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

\# 从 numpy 数组转换为 tensor
import numpy as np
b = np.array([[1, 2], [3, 4]])
c = torch.from_numpy(b)
```

\2. 矩阵加减：

\```python
\# 矩阵加法
a = torch.tensor([[1, 2], [3, 4]])
b = torch.tensor([[5, 6], [7, 8]])
c = a + b

\# 矩阵减法
d = a - b
\```

\3. 矩阵乘法：

\```python
\# 矩阵乘法
a = torch.tensor([[1, 2, 3], [4, 5, 6]])
b = torch.tensor([[7, 8], [9, 10], [11, 12]])
c = torch.mm(a, b)
\```

\4. 转置：

\```python
\# 转置矩阵
a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
b = torch.transpose(a, 0, 1)  # 对第一维和第二维进行交换
\```

\5. 求逆：

\```python
\# 求矩阵的逆
from torch.autograd import Variable
a = Variable(torch.tensor([[1, 2], [3, 4]]).float(), requires_grad=True)
a_inv = torch.inverse(a)
\```

\6. 矩阵切片：

\```python
\# 矩阵切片
a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
b = a[0:2, 0:2]
\```

