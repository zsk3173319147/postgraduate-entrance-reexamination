#### 神经网络中Batch Size的理解

> 直观的理解：
> Batch Size定义：一次训练所选取的样本数。
> Batch Size的大小影响模型的优化程度和速度。同时其直接影响到GPU内存的使用情况，假如你GPU内存不大，该数值最好设置小一点。
>
> 为什么要提出Batch Size？
> 在没有使用Batch Size之前，这意味着网络在训练时，是一次把所有的数据（整个数据库）输入网络中，然后计算它们的梯度进行反向传播，由于在计算梯度时使用了整个数据库，所以计算得到的梯度方向更为准确。但在这情况下，计算得到不同梯度值差别巨大，难以使用一个全局的学习率，所以这时一般使用Rprop这种基于梯度符号的训练算法，单独进行梯度更新。
> 在小样本数的数据库中，不使用Batch Size是可行的，而且效果也很好。但是一旦是大型的数据库，一次性把所有数据输进网络，肯定会引起内存的爆炸。所以就提出Batch Size的概念。
>
> Batch Size设置合适时的优点：
> 1、通过并行化提高内存的利用率。就是尽量让你的GPU满载运行，提高训练速度。
> 2、单个epoch的迭代次数减少了，参数的调整也慢了，假如要达到相同的识别精度，需要更多的epoch。
> 3、适当Batch Size使得梯度下降方向更加准确。
>
> Batch Size从小到大的变化对网络影响
> 1、没有Batch Size，梯度准确，只适用于小样本数据库
> 2、Batch Size=1，梯度变来变去，非常不准确，网络很难收敛。
> 3、Batch Size增大，梯度变准确，
> 4、Batch Size增大，梯度已经非常准确，再增加Batch Size也没有用
>
> 注意：Batch Size增大了，要到达相同的准确度，必须要增大epoch。

