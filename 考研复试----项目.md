### 基于GPT2模型的中文小说续写



#### GPT的全称为Generative Pre-trained Transformer

#### 项目名称：基于GPT-2的中文小说续写项目

**项目描述**：本项目是一个基于GPT-2的中文小说续写项目。项目目标是利用预训练的GPT-2模型来生成连续的中文小说文本，使其看起来像是由一个人写成的一样。通过此项目，我学习了GPT-2模型的原理、数据预处理、模型训练和生成技术。

**项目职责**：

- 下载和整理了大量中文小说数据使用Python编写了数据预处理代码，包括分词、文本清洗、转换成GPT-2模型所需的格式等
- 根据GPT-2模型的原理，调整了模型的一些参数，如学习率、批次大小、模型层数等
- 在GPU服务器上训练了GPT-2模型，并使用训练好的模型来生成中文小说续写文本

**项目成果**：

- 成功生成了一些看起来像是由一个人写成的中文小说续写文本，并且有一定的可读性

- 实现了数据预处理、模型训练和生成的流程，并成功生成了中文文章。

- 熟悉了GPT-2模型和transformer架构，并对模型的关键参数进行了调参。

  



1. #### 该项目的背景和目的是什么？

   > 这个项目基于GPT-2模型，旨在实现自动续写中文小说的功能。中文小说是中国传统文学的重要组成部分，受到了广大读者的热爱。但是，中文小说的创作需要耗费大量的人力和时间，因此自动续写中文小说的技术应运而生，能够极大地提高中文小说创作的效率。此项目的目的是探究基于GPT-2的中文小说续写技术，提高中文小说的创作效率和质量。

2. #### 您是如何构建和训练基于GPT2的语言模型的？使用了哪些技术？

   > 我使用了Python编程语言和PyTorch深度学习框架，构建了基于GPT-2的中文小说续写模型。在模型训练过程中，我从互联网上采集了一些经典中文小说的文本数据，并进行了数据清洗和预处理。然后，我使用了GPT-2模型对这些数据进行了训练，并对模型进行了优化和调参，以获得更好的续写效果。

3. #### 在该项目中，您主要负责了哪些工作？其中遇到了哪些困难和挑战？

   > 这个项目是我学习别人在github开源项目做的课程设计，我学习了该开源项目的代码实现、数据处理、模型训练等方面，并根据自己的理解和需要做了适当的修改和调整。遇到的困难有文件下载后转换的编码问题、怎么会转换成可以使用的json格式的文件，同时在训练的时候由于本地机的训练资源不足，转而使用colab的云服务资源，最大的问题是在训练自己的模型时，调参出现的问题。比如开始我的loss下降很慢，之后我发现每次训练都是从头开始，我觉得这样很浪费时间，就训练模型改成了上次最后训练完成的，后来我的loss不断在3和5之间震荡，怎么也下不去，想了一会，查了相关资料，可能有
   >
   > 学习率过大：如果学习率设置过大，模型参数更新时可能会“跳过”最优解，导致损失函数震荡。你可以尝试降低学习率。
   >
   > 数据集问题：如果数据集中存在错误标注或噪声数据，也可能导致损失函数震荡。你可以检查数据集并进行清洗。
   >
   > 模型问题：如果模型结构不合理或存在 bug，也可能导致损失函数震荡。你可以检查模型结构并修复 bug。
   >
   > 优化器问题：如果使用的优化器不合适，也可能导致损失函数震荡。你可以尝试更换优化器。
   >
   > 这四种可能。

4. #### 您觉得这个项目最有意义的成果是什么？它对于中文自然语言处理和中文文本生成有何贡献？

   > 最终，我们成功地实现了基于GPT-2的中文小说续写功能，并通过实验验证了该模型的准确性和可用性。该技术能够提高中文小说创作的效率和质量，为中文小说创作者和爱好者提供了便利。此外，该技术还可以应用于其他文本生成任务，比如自动作诗、机器翻译等。

5. #### 在该项目中，您对模型的选择和优化有何思考和建议？如果有更多的时间和资源，您会尝试改进哪些方面？

   > 可以考虑将其他领域的知识和信息融入到模型中，如情感分析、人物关系分析和文本摘要等，以提高模型的复杂度和多样性



#### 什么是语言模型？它的作用是什么？

> 语言模型（Language Model）是一种用于自然语言处理（NLP）的基本技术，它的主要作用是预测给定的一段文本中下一个可能出现的单词或字符，以此来衡量这段文本的语言质量。它可以被用来评估文本的连贯性、准确性、完整性等，同时也可以用于文本生成、自动翻译、语音识别、机器阅读理解等领域。语言模型是许多NLP任务的基础和核心，能够有效地提高自然语言处理的精度和效率。

#### 训练过程中的一些问题

1. ##### 我使用很小的数据集，之后由于没有cuda，使用cpu训练，训练200轮，loss也降到了0.03左右但是测试生成文章，全是乱码，这是为什么？

   > 如果你在训练过程中发现 loss 降到了一个较低的水平，但是在测试生成文章时却出现了乱码，可能有以下几个原因：
   >
   > 1. 过拟合：由于你使用的是一个很小的数据集，模型可能过度拟合训练数据，导致在测试数据上表现不佳。
   >
   > 2. 数据预处理问题：如果你在训练和测试时使用了不同的数据预处理方法，可能会导致模型无法正确解码生成的文本。
   >
   > 3. 模型结构或参数问题：如果模型结构或参数设置不当，可能会导致模型无法正确学习到文本生成的规律
   >
   >    我觉得1、和3有可能，而是不可能的，之后我得使用cuda训练大一点的数据集

2. 减少模型的层数（n_layer）可以减少模型的参数量。同时，减少 batch_size 可能需要增加 epoch 数量才能达到相同的训练效果。这两种方法都有可能减少过拟合（overfitting）的可能性。但是，具体的学习效果需要通过多次执行训练来观察，研究成效难以立竿见影

3. ##### 我的loss一直保持在3下不去？

   > 1. 数据集问题：可能数据集不够大或不够丰富，可以考虑增加数据量或使用更好的数据集；
   > 2. 模型结构问题：模型可能太简单，可以考虑增加模型的层数、宽度、改变激活函数等来提高模型复杂度；
   > 3. 参数调整问题：可以逐步减小学习率来帮助模型跳出局部最优解，或者调整其他超参数如batch_size、dropout等来提高模型的泛化能力；
   > 4. 训练策略问题：可以考虑使用更先进的优化器如AdamW、LAMB等，或者使用一些正则化方法如weight decay、early stopping等来防止过拟合。



#### 你的模型是如何生成中文小说的？

> 我的模型是基于GPT-2的中文小说生成模型，它的工作原理是利用前面的文本来预测下一个单词或字符，然后不断迭代生成下一个单词或字符，直到达到预设的生成长度或者生成出特定的结束标识符。
>
> 具体地说，当输入一段文本作为前缀时，模型会通过计算前缀与各个单词或字符之间的概率来预测下一个单词或字符，生成一个概率分布。接着，模型会根据这个概率分布来进行采样，从而得到下一个单词或字符，并将其作为下一步的输入，不断迭代生成直到达到预设的生成长度或结束标识符。
>
> 在生成过程中，我还可以控制模型的生成温度、top k采样等超参数，以控制模型生成文本的多样性和准确性。
>
> **生成温度是什么意思**？
>
> > 在基于语言模型的生成任务中，生成温度（temperature）是一个用来控制生成文本多样性的超参数。它决定了在每个时间步上，模型输出概率分布中，被选择的下一个字符的随机程度。当温度值较低时，模型更倾向于选择概率较大的字符，生成的文本相对保守和重复性较高；而当温度值较高时，模型更容易选择概率较小的字符，生成的文本更加多样化和创新性，但也可能更加不可预测。因此，在实际应用中，需要根据具体的生成任务和数据集特点，选择适当的温度值来平衡多样性和可控性。
>
> **那topk是什么？**
>
> Top-k是指在生成文本时，只考虑概率最高的前k个词。



#### 简单介绍一下transformer

> Transformer是一种用于序列到序列学习的深度学习模型，主要用于自然语言处理（NLP）任务，例如机器翻译、文本摘要、对话生成等。它是由谷歌在2017年提出的，并在其机器翻译系统中取得了显著的成果。
>
> Transformer 的核心思想是自注意力机制（self-attention），它能够在输入序列的所有位置上同时计算每个位置的表示。这种机制可以将每个位置的表示与其他位置的表示相关联，以便在编码和解码阶段捕获更丰富的语义信息。
>
> 在 Transformer 中，有两个主要的模块：编码器和解码器。编码器将输入序列转换为上下文相关的表示，解码器根据编码器的输出和当前的状态生成输出序列。
>
> 每个编码器和解码器都由多个相同的层组成，每个层都包括两个子层：一个是多头自注意力子层，另一个是全连接前馈子层。
>
> 在多头自注意力子层中，输入序列中的每个位置都会与所有其他位置进行关联。它使用多个注意力头来计算注意力矩阵，以便在不同的表示空间中捕获不同的语义信息。
>
> 在全连接前馈子层中，每个位置的表示都通过两个线性变换和激活函数进行转换。这个子层是为了在每个位置上学习一个非线性函数，以便更好地捕获语义信息。
>
> 在解码器中，还有一个额外的注意力子层，称为编码器-解码器注意力子层，用于在生成每个输出时对输入序列进行关注



#### 什么是自注意力机制

> 自注意力机制是Transformer模型的一个关键组成部分，也是它的创新之处之一。简单来说，自注意力机制是一种基于输入序列自身的注意力机制。
>
> 在自注意力机制中，每个输入序列的元素（比如句子中的每个单词）都会被表示为一个向量，该向量将被用于计算与其他向量之间的相似度。这个相似度可以看作是每个元素与整个序列的关联程度。使用这个关联程度，我们可以根据输入序列中的其他元素来加权计算每个元素的表示，从而捕获全局信息。
>
> 在Transformer中，自注意力机制被用于同时计算输入序列的编码和解码。通过多层自注意力机制和前馈神经网络，Transformer模型能够在不引入循环或卷积操作的情况下，对序列数据进行建模和处理。这使得Transformer模型能够在并行计算中更有效地处理长序列数据，



#### GPT-2模型是什么？它有哪些特点和优势？

> GPT-2是一种基于深度学习的语言模型，全称为Generative Pre-trained Transformer 2。它是OpenAI公司在2019年发布的一个预训练模型，采用了Transformer网络结构。GPT-2模型可以对一段文本进行预测，给出下一个可能的单词或词组，从而实现对文本的自动生成。
>
> GPT-2模型的主要特点和优势包括：
>
> 1. 预训练模型：GPT-2模型采用预训练的方式，先在大规模语料库上进行训练，然后通过微调等方式应用到具体的任务中。
> 2. Transformer网络结构：GPT-2模型采用了Transformer网络结构，该结构能够自适应地对输入文本进行编码，同时利用多头注意力机制来捕捉文本中的长程依赖关系，从而提高了模型的性能。
> 3. 多样性生成：GPT-2模型能够在一定程度上实现文本的多样性生成，即在给定的前缀下，生成多个不同的、但语法和语义都合理的后续文本。
> 4. 零样本学习：GPT-2模型可以实现零样本学习，即在没有任何额外训练数据的情况下，生成与训练数据相似的文本。





#### 怎么评价你的模型的好坏？

- Perplexity：可以通过对测试数据集运行模型并计算每个句子的交叉熵得到。最终的perplexity是交叉熵的指数，用来表示模型对于新数据的预测能力。具体计算方法可以参考：https://en.wikipedia.org/wiki/Perplexity
- BLEU：是机器翻译任务中广泛使用的指标，可以用来评估生成文本的质量。它通过比较生成文本和参考文本之间的n-gram重叠程度来计算。具体计算方法可以参考：https://en.wikipedia.org/wiki/BLEU
- ROUGE：也是机器翻译任务中常用的指标之一，可以用来评估生成文本和参考文本之间的相似度。具体计算方法可以参考：https://en.wikipedia.org/wiki/ROUGE_(metric)
- F1 Score：在一些分类任务中常用的指标，可以用来评估模型在预测正例和负例时的准确率和召回率的平衡情况。具体计算方法可以参考：https://en.wikipedia.org/wiki/F1_score

#### 在基于GPT-2的中文小说续写项目中，模型评估和优化技术是非常重要的一环。下面介绍一些常用的技术：

1. 损失函数：损失函数是用来衡量模型预测结果与实际结果之间的差距，常见的损失函数有交叉熵、均方误差、对数损失等。选择合适的损失函数可以提高模型的预测精度。
2. 评价指标：评价指标用来衡量模型预测结果的好坏，如准确率、召回率、F1值等。选择合适的评价指标可以更直观地反映模型的效果。
3. 交叉验证：交叉验证是一种通过对数据集进行多次划分训练和测试来评估模型性能的方法。交叉验证可以充分利用数据集，避免过拟合和欠拟合等问题。
4. 正则化：正则化是一种通过加入正则项来限制模型复杂度的方法，如L1正则化、L2正则化等。正则化可以避免模型过拟合和提高模型泛化能力。
5. 超参数优化：超参数是指模型中需要手动设置的参数，如学习率、批大小、隐藏层数等。通过对超参数进行优化，可以提高模型的性能。
6. 梯度下降算法：梯度下降是一种优化算法，通过反向传播计算梯度来更新模型参数，以最小化损失函数。不同的梯度下降算法有不同的优缺点，需要根据具体情况选择合适的算法。



#### BERT和GPT区别

- GPT 选择 Transformer 里的 **Decoder**，训练目标为一般的语言模型，预测下个字
- BERT 选择 Transformer 里的 **Encoder**，训练目标则为克漏字填空以及下句预测





#### 神经网络中Batch Size的理解

> 直观的理解：
> Batch Size定义：一次训练所选取的样本数。
> Batch Size的大小影响模型的优化程度和速度。同时其直接影响到GPU内存的使用情况，假如你GPU内存不大，该数值最好设置小一点。
>
> 为什么要提出Batch Size？
> 在没有使用Batch Size之前，这意味着网络在训练时，是一次把所有的数据（整个数据库）输入网络中，然后计算它们的梯度进行反向传播，由于在计算梯度时使用了整个数据库，所以计算得到的梯度方向更为准确。但在这情况下，计算得到不同梯度值差别巨大，难以使用一个全局的学习率，所以这时一般使用Rprop这种基于梯度符号的训练算法，单独进行梯度更新。
> 在小样本数的数据库中，不使用Batch Size是可行的，而且效果也很好。但是一旦是大型的数据库，一次性把所有数据输进网络，肯定会引起内存的爆炸。所以就提出Batch Size的概念。
>
> Batch Size设置合适时的优点：
> 1、通过并行化提高内存的利用率。就是尽量让你的GPU满载运行，提高训练速度。
> 2、单个epoch的迭代次数减少了，参数的调整也慢了，假如要达到相同的识别精度，需要更多的epoch。
> 3、适当Batch Size使得梯度下降方向更加准确。
>
> Batch Size从小到大的变化对网络影响
> 1、没有Batch Size，梯度准确，只适用于小样本数据库
> 2、Batch Size=1，梯度变来变去，非常不准确，网络很难收敛。
> 3、Batch Size增大，梯度变准确，
> 4、Batch Size增大，梯度已经非常准确，再增加Batch Size也没有用
>
> 注意：Batch Size增大了，要到达相同的准确度，必须要增大epoch。

